apiVersion: networking.istio.io/v1
kind: DestinationRule
metadata:
  name: granite3-8b-epp
spec:
  host: granite3-8b-epp.sza.svc.cluster.local
  trafficPolicy:
    tls:
      insecureSkipVerify: true
      mode: SIMPLE
---
apiVersion: inference.networking.x-k8s.io/v1alpha2
kind: InferencePool
metadata:
  name: granite3-8b-epp
  annotations:
    activator.llm-d.ai/target-apiversion: apps/v1
    activator.llm-d.ai/target-kind: Deployment
    activator.llm-d.ai/target-name: granite3-8b
spec:
  extensionRef:
    failureMode: FailClose
    group: ""
    kind: Service
    name: granite3-8b-epp
    portNumber: 9002
  selector:
    llm-d.ai/model: "granite3-8b"
    llm-d.ai/inferenceServing: "true"
  targetPortNumber: 8000
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: granite3-8b-epp
  labels:
    app: granite3-8b-epp
spec:
  replicas: 1
  selector:
    matchLabels:
      inferencepool: granite3-8b-epp
  template:
    metadata:
      labels:
        inferencepool: granite3-8b-epp
    spec:
      containers:
      - name: epp
        image: ghcr.io/llm-d/llm-d-inference-scheduler:v0.3.2
        args:
        - --pool-name
        - granite3-8b-epp
        - --pool-namespace
        - sza
        - --pool-group
        - inference.networking.x-k8s.io
        - --zap-encoder
        - json
        - --config-file
        - /config/default-plugins.yaml
        - --v
        - "4"
        env:
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        livenessProbe:
          failureThreshold: 3
          grpc:
            port: 9003
            service: inference-extension
          initialDelaySeconds: 1
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        ports:
        - containerPort: 9002
          name: grpc
          protocol: TCP
        - containerPort: 9003
          name: grpc-health
          protocol: TCP
        - containerPort: 9090
          name: metrics
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          grpc:
            port: 9003
            service: inference-extension
          periodSeconds: 2
          successThreshold: 1
          timeoutSeconds: 1
        volumeMounts:
        - mountPath: /config
          name: plugins-config-volume
      volumes:
      - configMap:
          defaultMode: 420
          name: epp
        name: plugins-config-volume
---
apiVersion: v1
kind: Service
metadata:
  name: granite3-8b-epp
spec:
  selector:
    inferencepool: granite3-8b-epp
  ports:
  - name: grpc-ext-proc
    port: 9002
    protocol: TCP
    targetPort: 9002
  - name: http-metrics
    port: 9090
    protocol: TCP
    targetPort: 9090
  type: ClusterIP

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: granite3-8b
spec:
  replicas: 0 # for scale from zero
  selector:
    matchLabels:
      llm-d.ai/model: "granite3-8b"
      llm-d.ai/inferenceServing: "true"
  template:
    metadata:
      labels:
        llm-d.ai/model: "granite3-8b"
        llm-d.ai/inferenceServing: "true"
    spec:
      containers:
      - args:
        - --model
        - granite/granite3-8B
        - --port
        - "8000"
        image: ghcr.io/llm-d/llm-d-inference-sim:v0.5.1
        imagePullPolicy: IfNotPresent
        name: vllm-sim
        env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
        ports:
        - containerPort: 8000
          name: http
          protocol: TCP
        readinessProbe:
          httpGet:
             path: /health
             port: 8000
          initialDelaySeconds: 10 # simulate vllm startup time (optimized)
          periodSeconds: 5
