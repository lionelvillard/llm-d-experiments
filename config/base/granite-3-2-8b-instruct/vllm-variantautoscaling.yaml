apiVersion: llmd.ai/v1alpha1
kind: VariantAutoscaling
metadata:
  name:  granite-3-2-8b-instruct
  labels:
    inference.optimization/acceleratorName: "NVIDIA-H100-80GB-HBM3"
spec:
  targetVariantRefs:
  - apiVersion: apps/v1
    kind: Deployment
    name: granite-3-2-8b-instruct
  modelID: granite/granite-3-2-8b-instruct
  sloClassRef:
    # Configmap name to load in the same namespace as optimizer object
    # we start with static (non-changing) ConfigMaps (for ease of implementation only)
    name: freemium
    # Key (modelID) present inside configmap
    key: granite/granite-3-2-8b-instruct
  # Static profiled benchmarked data for a variant running on different accelerators
  modelProfile:
    accelerators:
      - acc: "L40S"
        accCount: 1
        perfParms:
          decodeParms:
            alpha: "22.619"
            beta: "0.181"
          prefillParms:
            gamma: "226.19"
            delta: "0.018"
        maxBatchSize: 512
      - acc: "H100"
        accCount: 1
        perfParms:
          decodeParms:
            # Decode parameters for ITL equation: itl = alpha + beta * maxBatchSize
            alpha: "7.470"
            beta: "0.044"
          # Prefill parameters for TTFT equation: ttft = gamma + delta * tokens * maxBatchSize
          prefillParms:
            gamma: "15.415"
            delta: "0.000337"
        maxBatchSize: 512
      - acc: "A100"
        accCount: 1
        perfParms:
          decodeParms:
            # Decode parameters for ITL equation: itl = alpha + beta * maxBatchSize
            alpha: "20.58"
            beta: "0.41"
          # Prefill parameters for TTFT equation: ttft = gamma + delta * tokens * maxBatchSize
          prefillParms:
            gamma: "5.2"
            delta: "0.1"
        maxBatchSize: 4
